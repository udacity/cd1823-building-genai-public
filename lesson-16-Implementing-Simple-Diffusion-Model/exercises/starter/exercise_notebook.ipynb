{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "218fd2bc",
   "metadata": {},
   "source": [
    "# Exercise: Training a Simple DDPM on MNIST\n",
    "\n",
    "**Objective:** Implement and train a Denoising Diffusion Probabilistic Model (DDPM) on MNIST with smooth MSE convergence.\n",
    "\n",
    "**Key Learning:**\n",
    "- Understand noise schedules and forward diffusion\n",
    "- Implement time embeddings for diffusion conditioning\n",
    "- Build a U-Net for noise prediction\n",
    "- Train with MSE loss (demonstrating stable convergence vs GANs)\n",
    "- Analyze convergence curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee38f6",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import necessary libraries for building and training the diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de64dac",
   "metadata": {},
   "source": [
    "## Section 2: Load and Prepare MNIST Dataset\n",
    "\n",
    "Load MNIST, normalize to [-1, 1], create data loaders, and visualize samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: Normalize to [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Load MNIST\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "\n",
    "# Visualize a few samples\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    img, label = train_dataset[i]\n",
    "    axes[i].imshow(img.squeeze(), cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ea67b",
   "metadata": {},
   "source": [
    "## Section 3: TODO 1 - Define Noise Scheduler\n",
    "\n",
    "**TODO 1:** Complete the `NoiseScheduler.__init__` method.\n",
    "\n",
    "Implement linear beta schedule, compute alphas, cumulative products, and pre-compute square roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    \"\"\"Noise scheduler for forward diffusion process.\"\"\"\n",
    "\n",
    "    def __init__(self, num_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        \"\"\"\n",
    "        TODO 1: Initialize noise scheduler.\n",
    "\n",
    "        Requirements:\n",
    "        1. Create linear beta schedule from beta_start to beta_end\n",
    "        2. Compute alphas (1 - beta)\n",
    "        3. Compute cumulative products (alphas_cumprod)\n",
    "        4. Pre-compute square roots for efficiency\n",
    "\n",
    "        Mathematical formulas:\n",
    "        - β_t: Linear schedule from beta_start to beta_end\n",
    "        - α_t = 1 - β_t\n",
    "        - ᾱ_t = ∏_{i=0}^t α_i (cumulative product)\n",
    "        - √ᾱ_t and √(1-ᾱ_t) are used in forward diffusion formula\n",
    "        \"\"\"\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # TODO 1: Replace pass with implementation\n",
    "        pass\n",
    "\n",
    "    def get_coefficients(self, timesteps):\n",
    "        \"\"\"Get pre-computed coefficients for a batch of timesteps.\"\"\"\n",
    "        sqrt_alphas = self.sqrt_alphas_cumprod[timesteps]\n",
    "        sqrt_one_minus_alphas = self.sqrt_one_minus_alphas_cumprod[timesteps]\n",
    "\n",
    "        # Reshape for broadcasting\n",
    "        if len(sqrt_alphas.shape) == 1:\n",
    "            sqrt_alphas = sqrt_alphas[:, None, None, None]\n",
    "            sqrt_one_minus_alphas = sqrt_one_minus_alphas[:, None, None, None]\n",
    "\n",
    "        return sqrt_alphas, sqrt_one_minus_alphas\n",
    "\n",
    "\n",
    "# Test the scheduler\n",
    "try:\n",
    "    scheduler = NoiseScheduler()\n",
    "    print(\"✓ NoiseScheduler initialized\")\n",
    "    print(f\"  Timesteps: {scheduler.num_timesteps}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5298d",
   "metadata": {},
   "source": [
    "## Section 4: TODO 2 - Forward Diffusion Process\n",
    "\n",
    "**TODO 2:** Implement the `add_noise` function for forward diffusion.\n",
    "\n",
    "Use the formula: $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf922c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(x_0, timestep, scheduler, noise=None):\n",
    "    \"\"\"\n",
    "    TODO 2: Add noise to images (forward diffusion).\n",
    "\n",
    "    Formula: x_t = √ᾱ_t * x_0 + √(1-ᾱ_t) * ε\n",
    "\n",
    "    Args:\n",
    "        x_0: Original images (batch, 1, 28, 28)\n",
    "        timestep: Timesteps (batch,)\n",
    "        scheduler: NoiseScheduler instance\n",
    "        noise: Optional pre-generated noise\n",
    "\n",
    "    Returns:\n",
    "        x_t: Noisy images (batch, 1, 28, 28)\n",
    "        noise: The noise added (for training)\n",
    "\n",
    "    Implementation steps:\n",
    "    1. If noise is None, create random noise using torch.randn_like()\n",
    "    2. Get coefficients from scheduler using get_coefficients()\n",
    "    3. Apply forward diffusion formula\n",
    "    4. Return noisy image and noise\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO 2: Replace pass with implementation\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test forward diffusion\n",
    "try:\n",
    "    x_0 = torch.randn(4, 1, 28, 28)\n",
    "    t = torch.tensor([0, 250, 500, 999])\n",
    "    x_t, noise = add_noise(x_0, t, scheduler)\n",
    "    print(\"✓ Forward diffusion working\")\n",
    "    print(f\"  x_0 shape: {x_0.shape}, range: [{x_0.min():.2f}, {x_0.max():.2f}]\")\n",
    "    print(f\"  x_t shape: {x_t.shape}, range: [{x_t.min():.2f}, {x_t.max():.2f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e12b79",
   "metadata": {},
   "source": [
    "## Section 5: TODO 3-5 - Build U-Net Architecture\n",
    "\n",
    "**TODO 3:** Implement `TimeEmbedding` with sinusoidal encoding\n",
    "**TODO 4:** Implement `ResidualBlock` with FiLM time conditioning\n",
    "**TODO 5:** Implement `SimpleUNet` forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"TODO 3: Encode timestep into sinusoidal embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, timestep):\n",
    "        \"\"\"\n",
    "        TODO 3: Implement sinusoidal time embedding.\n",
    "\n",
    "        Formula creates frequency schedule:\n",
    "        freqs = exp(-ln(10000) * k / d) for k in [0, d)\n",
    "\n",
    "        Then embedding = [sin(t * freqs), cos(t * freqs)]\n",
    "\n",
    "        Args:\n",
    "            timestep: LongTensor of shape (batch,)\n",
    "\n",
    "        Returns:\n",
    "            embedding: FloatTensor of shape (batch, embedding_dim)\n",
    "        \"\"\"\n",
    "        device = timestep.device\n",
    "        half_dim = self.embedding_dim // 2\n",
    "\n",
    "        # TODO 3: Replace pass with implementation\n",
    "        pass\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"TODO 4: Residual block with FiLM time conditioning.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, embedding_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # FiLM: Feature-wise Linear Modulation\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, out_channels * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_channels * 2, out_channels),\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, time_embedding):\n",
    "        \"\"\"\n",
    "        TODO 4: Implement FiLM-modulated residual block.\n",
    "\n",
    "        FiLM applies time-dependent scaling: h = h * time_scale\n",
    "\n",
    "        Implementation:\n",
    "        1. Apply norm1 → SiLU → conv1\n",
    "        2. Compute time_scale via time_mlp\n",
    "        3. Reshape time_scale to (batch, channels, 1, 1)\n",
    "        4. Apply FiLM: h = h * time_scale\n",
    "        5. Apply norm2 → SiLU → conv2\n",
    "        6. Add skip connection: h + skip(x)\n",
    "        \"\"\"\n",
    "        # TODO 4: Replace pass with implementation\n",
    "        pass\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"TODO 5: Simple U-Net for noise prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, image_channels=1, base_channels=64):\n",
    "        super().__init__()\n",
    "        self.time_embedding = TimeEmbedding(embedding_dim=128)\n",
    "\n",
    "        self.init_conv = nn.Conv2d(\n",
    "            image_channels, base_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        # Encoder (downsampling)\n",
    "        self.down_res1 = ResidualBlock(base_channels, base_channels, embedding_dim=128)\n",
    "        self.down_conv1 = nn.Conv2d(\n",
    "            base_channels, base_channels, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "\n",
    "        self.down_res2 = ResidualBlock(\n",
    "            base_channels, base_channels * 2, embedding_dim=128\n",
    "        )\n",
    "        self.down_conv2 = nn.Conv2d(\n",
    "            base_channels * 2, base_channels * 2, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.middle_res = ResidualBlock(\n",
    "            base_channels * 2, base_channels * 2, embedding_dim=128\n",
    "        )\n",
    "\n",
    "        # Decoder (upsampling)\n",
    "        self.up_conv2 = nn.ConvTranspose2d(\n",
    "            base_channels * 2, base_channels, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "        self.up_res2 = ResidualBlock(base_channels, base_channels, embedding_dim=128)\n",
    "\n",
    "        self.up_conv1 = nn.ConvTranspose2d(\n",
    "            base_channels, base_channels, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "        self.up_res1 = ResidualBlock(base_channels, base_channels, embedding_dim=128)\n",
    "\n",
    "        # Final\n",
    "        self.final_norm = nn.GroupNorm(32, base_channels)\n",
    "        self.final_conv = nn.Conv2d(\n",
    "            base_channels, image_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        \"\"\"\n",
    "        TODO 5: Implement U-Net forward pass.\n",
    "\n",
    "        Architecture:\n",
    "        Input (28×28) → init_conv (64 channels)\n",
    "        → Encoder: down_res1 → down_conv1 (28→14)\n",
    "                 → down_res2 → down_conv2 (14→7)\n",
    "        → Middle: middle_res (7×7)\n",
    "        → Decoder: up_conv2 (7→14)\n",
    "                 → up_res2 → up_conv1 (14→28)\n",
    "                 → up_res1\n",
    "        → Final: final_conv (1 channel)\n",
    "\n",
    "        Pass time_embedding to each ResidualBlock.\n",
    "        \"\"\"\n",
    "        time_emb = self.time_embedding(timestep)\n",
    "\n",
    "        # TODO 5: Replace pass with implementation\n",
    "        pass\n",
    "\n",
    "\n",
    "# Test U-Net\n",
    "try:\n",
    "    model = SimpleUNet().to(device)\n",
    "    x = torch.randn(4, 1, 28, 28).to(device)\n",
    "    t = torch.tensor([0, 250, 500, 999]).to(device)\n",
    "    out = model(x, t)\n",
    "    print(\"U-Net forward pass working\")\n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Output shape: {out.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1d665",
   "metadata": {},
   "source": [
    "## Section 6: TODO 6 - Training Step\n",
    "\n",
    "**TODO 6:** Implement single training iteration with MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c90b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, x_0, scheduler, device):\n",
    "    \"\"\"\n",
    "    TODO 6: Single training iteration.\n",
    "\n",
    "    Steps:\n",
    "    1. Move data to device\n",
    "    2. Sample random timesteps\n",
    "    3. Sample random noise\n",
    "    4. Apply forward diffusion to get x_t\n",
    "    5. Predict noise with model\n",
    "    6. Compute MSE loss\n",
    "    7. Backward pass and step\n",
    "\n",
    "    Returns:\n",
    "        loss_value: MSE loss for this batch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    x_0 = x_0.to(device)\n",
    "    batch_size = x_0.shape[0]\n",
    "\n",
    "    # TODO 6: Replace pass with implementation\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test train_step\n",
    "try:\n",
    "    model = SimpleUNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    batch = torch.randn(4, 1, 28, 28)\n",
    "    loss = train_step(model, optimizer, batch, scheduler, device)\n",
    "    print(\"✓ Training step working\")\n",
    "    print(f\"  Loss: {loss:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e7687",
   "metadata": {},
   "source": [
    "## Section 7: TODO 7 - Training Loop\n",
    "\n",
    "**TODO 7:** Implement training for one epoch (loop over all batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, scheduler, device):\n",
    "    \"\"\"\n",
    "    TODO 7: Train for one epoch.\n",
    "\n",
    "    Loop through all batches:\n",
    "    1. Call train_step for each batch\n",
    "    2. Accumulate losses\n",
    "    3. Return average loss\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average MSE loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # TODO 7: Replace pass with implementation\n",
    "    pass\n",
    "\n",
    "\n",
    "print(\"train_epoch function defined (TODO 7)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afbbf6",
   "metadata": {},
   "source": [
    "## Section 8: TODO 8 - Complete Training Script\n",
    "\n",
    "**TODO 8:** Implement main training loop with device setup and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62823823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    TODO 8: Complete training script.\n",
    "\n",
    "    Steps:\n",
    "    1. Device selection (already done, use global 'device')\n",
    "    2. Create model\n",
    "    3. Setup optimizer (learning rate = 0.001)\n",
    "    4. Training loop for 10 epochs\n",
    "    5. Plot and analyze convergence\n",
    "\n",
    "    Key point: Observe SMOOTH convergence (unlike cGAN)\n",
    "    \"\"\"\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = SimpleUNet(image_channels=1, base_channels=64).to(device)\n",
    "    scheduler_obj = NoiseScheduler(num_timesteps=1000)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    num_epochs = 10\n",
    "    epoch_losses = []\n",
    "\n",
    "    print(\"\\nStarting DDPM Training (10 epochs)...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # TODO 8: Replace pass with implementation\n",
    "    pass\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(\n",
    "        range(1, num_epochs + 1), epoch_losses, marker=\"o\", linewidth=2, markersize=8\n",
    "    )\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "    plt.title(\"DDPM: Smooth Convergence\\n(Compare with cGAN volatility)\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Analysis\n",
    "    plt.subplot(1, 2, 2)\n",
    "    diffs = [\n",
    "        abs(epoch_losses[i + 1] - epoch_losses[i]) for i in range(len(epoch_losses) - 1)\n",
    "    ]\n",
    "    plt.bar(range(1, len(diffs) + 1), diffs, alpha=0.7)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Absolute Loss Change\", fontsize=12)\n",
    "    plt.title(\"Loss Stability\\n(Small changes = Smooth convergence)\", fontsize=12)\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print analysis\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Convergence Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Initial Loss: {epoch_losses[0]:.6f}\")\n",
    "    print(f\"Final Loss:   {epoch_losses[-1]:.6f}\")\n",
    "    improvement = (epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100\n",
    "    print(f\"Improvement:  {improvement:.1f}%\")\n",
    "    print(f\"Max jump:     {max(diffs):.6f}\")\n",
    "    print(f\"Avg jump:     {sum(diffs)/len(diffs):.6f}\")\n",
    "\n",
    "    if max(diffs) < 0.1 * epoch_losses[0]:\n",
    "        print(\"\\n Convergence is SMOOTH!\")\n",
    "    else:\n",
    "        print(\"\\n High volatility detected\")\n",
    "\n",
    "\n",
    "# Run training\n",
    "# Note: Uncomment the line below to run training\n",
    "# main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "918daddb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Learning Points:**\n",
    "\n",
    "1. **Stable Convergence:** MSE loss guarantees smooth, monotonic convergence\n",
    "2. **Noise Schedules:** Fixed schedule (no learning) simplifies training\n",
    "3. **Time Conditioning:** Embedding enables diffusion at all timesteps\n",
    "4. **U-Net Architecture:** Encoder-decoder preserves spatial information\n",
    "5. **Comparison with cGAN:** Diffusion is fundamentally more stable\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Train for 50+ epochs for better quality\n",
    "- Implement conditional generation (add class labels)\n",
    "- Try different noise schedules (cosine, sigmoid)\n",
    "- Compare with Module 13's cGAN volatility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55207f7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
