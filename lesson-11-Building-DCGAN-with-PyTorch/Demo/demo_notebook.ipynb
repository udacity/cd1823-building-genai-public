{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96ec1af7",
   "metadata": {},
   "source": [
    "# Module 11: Building and Training a DCGAN with PyTorch\n",
    "\n",
    "**Learning Objective**: Understand and implement Deep Convolutional GANs (DCGANs) using convolutional layers instead of MLPs, and train on CIFAR-10 for higher-resolution image generation.\n",
    "\n",
    "## DCGAN Overview\n",
    "- **DCGAN** = Deep Convolutional Generative Adversarial Network\n",
    "- Replaces MLP layers with **ConvTranspose2d** (Generator) and **Conv2d** (Discriminator)\n",
    "- Adheres to architectural guidelines: Batch Normalization, LeakyReLU, strided convolutions\n",
    "- Designed for **32×32 RGB images** (CIFAR-10)\n",
    "- Produces **higher-quality, more structured images** compared to MLP GANs\n",
    "\n",
    "Key Improvements:\n",
    "+ Convolutional layers preserve spatial structure\n",
    "+ Batch Normalization for stable training\n",
    "+ Strided convolutions for efficient upsampling/downsampling\n",
    "+ Better gradient flow and faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbae9f7",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Import DCGAN models from local files\n",
    "from models.dcgan import (\n",
    "    DCGANGenerator,\n",
    "    DCGANDiscriminator,\n",
    "    create_dcgan_models,\n",
    "    print_model_summary,\n",
    ")\n",
    "from dcgan_training import DCGANTrainer, initialize_weights\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Determine device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"✓ All imports successful!\")\n",
    "print(f\"✓ Using device: {device}\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa52cc",
   "metadata": {},
   "source": [
    "## Part 2: Load and Preprocess CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms: normalize to [-1, 1] for Tanh output\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)\n",
    "        ),  # Normalize to [-1, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Downloading and loading CIFAR-10 dataset...\")\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=(\n",
    "        2 if device.type != \"mps\" else 0\n",
    "    ),  # MPS doesn't work well with multiprocessing\n",
    ")\n",
    "\n",
    "print(f\" CIFAR-10 dataset loaded\")\n",
    "print(f\"  Total training images: {len(train_dataset)}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Image shape: (3, 32, 32) - RGB, 32×32 pixels\")\n",
    "\n",
    "# Visualize sample images\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i in range(16):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    img = sample_batch[i]\n",
    "    # Denormalize for visualization\n",
    "    img = (img + 1) / 2  # [-1, 1] → [0, 1]\n",
    "    ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"CIFAR-10 Sample Images (Training Set)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"\\nClass mapping: 0=airplane, 1=automobile, 2=bird, 3=cat, 4=deer, 5=dog, 6=frog, 7=horse, 8=ship, 9=truck\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2887a5",
   "metadata": {},
   "source": [
    "## Part 3: Understanding DCGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DCGAN architecture\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Generator architecture\n",
    "ax = axes[0]\n",
    "ax.set_title(\"DCGAN Generator\\n(Noise → Image)\", fontsize=13, fontweight=\"bold\")\n",
    "gen_layers = [\n",
    "    \"Input: z (100)\",\n",
    "    \"FC: 100 → 256×4×4\",\n",
    "    \"ConvTranspose: 256→128 (4→8)\",\n",
    "    \"ConvTranspose: 128→64 (8→16)\",\n",
    "    \"ConvTranspose: 64→32 (16→32)\",\n",
    "    \"ConvTranspose: 32→3 (32→32)\",\n",
    "    \"Output: (3, 32, 32)\",\n",
    "]\n",
    "y_pos = np.linspace(0.95, 0.05, len(gen_layers))\n",
    "colors_gen = [\n",
    "    \"lightblue\",\n",
    "    \"lightgreen\",\n",
    "    \"lightyellow\",\n",
    "    \"lightyellow\",\n",
    "    \"lightyellow\",\n",
    "    \"lightyellow\",\n",
    "    \"lightcoral\",\n",
    "]\n",
    "for i, (layer, y, color) in enumerate(zip(gen_layers, y_pos, colors_gen)):\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        y,\n",
    "        layer,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=color, alpha=0.8, pad=0.8),\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    if i < len(gen_layers) - 1:\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(0.5, y_pos[i + 1] + 0.02),\n",
    "            xytext=(0.5, y - 0.02),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2, transform=ax.transAxes),\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Discriminator architecture\n",
    "ax = axes[1]\n",
    "ax.set_title(\n",
    "    \"DCGAN Discriminator\\n(Image → Classification)\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "disc_layers = [\n",
    "    \"Input: (3, 32, 32)\",\n",
    "    \"Conv: 3→32 (32→16)\",\n",
    "    \"Conv: 32→64 (16→8)\",\n",
    "    \"Conv: 64→128 (8→4)\",\n",
    "    \"Conv: 128→256 (4→2)\",\n",
    "    \"Flatten: 256×2×2\",\n",
    "    \"FC: 1024 → 1\",\n",
    "    \"Output: probability\",\n",
    "]\n",
    "y_pos = np.linspace(0.95, 0.05, len(disc_layers))\n",
    "colors_disc = [\n",
    "    \"lightcoral\",\n",
    "    \"lightyellow\",\n",
    "    \"lightyellow\",\n",
    "    \"lightyellow\",\n",
    "    \"lightyellow\",\n",
    "    \"lightgreen\",\n",
    "    \"lightblue\",\n",
    "    \"lightgreen\",\n",
    "]\n",
    "for i, (layer, y, color) in enumerate(zip(disc_layers, y_pos, colors_disc)):\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        y,\n",
    "        layer,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=color, alpha=0.8, pad=0.8),\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    if i < len(disc_layers) - 1:\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(0.5, y_pos[i + 1] + 0.02),\n",
    "            xytext=(0.5, y - 0.02),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2, transform=ax.transAxes),\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Key DCGAN Principles:\")\n",
    "print(\"━\" * 80)\n",
    "print(\"1. ConvTranspose2d (Generator): Fractional-strided convolutions for upsampling\")\n",
    "print(\"   - Transforms (100,) latent vector → (256, 4, 4) → (3, 32, 32)\")\n",
    "print(\"   - Stride=2 doubles spatial dimensions at each layer\")\n",
    "print(\"\")\n",
    "print(\"2. Conv2d (Discriminator): Strided convolutions for downsampling\")\n",
    "print(\"   - Transforms (3, 32, 32) → (256, 2, 2) → (1,) probability\")\n",
    "print(\"   - Stride=2 halves spatial dimensions at each layer\")\n",
    "print(\"\")\n",
    "print(\"3. Batch Normalization: Stabilizes training\")\n",
    "print(\"   - Applied to all layers except output\")\n",
    "print(\"   - Helps with gradient flow and convergence\")\n",
    "print(\"\")\n",
    "print(\"4. Activation Functions:\")\n",
    "print(\"   - Generator: ReLU (hidden), Tanh (output)\")\n",
    "print(\"   - Discriminator: LeakyReLU(0.2) for stable gradients\")\n",
    "print(\"━\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1602225",
   "metadata": {},
   "source": [
    "## Part 4: Create and Initialize DCGAN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d278bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DCGAN models\n",
    "latent_dim = 100\n",
    "num_channels = 3\n",
    "\n",
    "generator, discriminator = create_dcgan_models(\n",
    "    latent_dim=latent_dim,\n",
    "    num_channels=num_channels,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Initialize weights (important for DCGAN!)\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)\n",
    "\n",
    "# Print model summary\n",
    "print_model_summary(generator, discriminator, latent_dim)\n",
    "\n",
    "# Test forward passes\n",
    "print(\"\\n Testing Forward Passes:\")\n",
    "print(\"━\" * 80)\n",
    "batch_size = 4\n",
    "z = torch.randn(batch_size, latent_dim, device=device)\n",
    "fake_images = generator(z)\n",
    "print(f\"Generator Input (noise):     {z.shape} → {fake_images.shape}\")\n",
    "\n",
    "D_output = discriminator(fake_images)\n",
    "print(f\"Discriminator Input (images): {fake_images.shape} → {D_output.shape}\")\n",
    "print(\"━\" * 80)\n",
    "print(\" All shape assertions passed!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22819d",
   "metadata": {},
   "source": [
    "## Part 5: Training Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44da9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "lr_g = 0.0002\n",
    "lr_d = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Create trainer\n",
    "trainer = DCGANTrainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    device=device,\n",
    "    lr_g=lr_g,\n",
    "    lr_d=lr_d,\n",
    "    beta1=beta1,\n",
    "    beta2=beta2,\n",
    ")\n",
    "\n",
    "print(\"DCGAN Training Configuration:\")\n",
    "print(\"━\" * 80)\n",
    "print(f\"Number of Epochs:        {num_epochs}\")\n",
    "print(f\"Generator LR:            {lr_g}\")\n",
    "print(f\"Discriminator LR:        {lr_d}\")\n",
    "print(f\"Adam Beta1:              {beta1}\")\n",
    "print(f\"Adam Beta2:              {beta2}\")\n",
    "print(f\"Batch Size:              {batch_size}\")\n",
    "print(f\"Latent Dimension:        {latent_dim}\")\n",
    "print(f\"Dataset Size:            {len(train_dataset)}\")\n",
    "print(f\"Total Batches:           {len(train_loader)}\")\n",
    "print(f\"Device:                  {device}\")\n",
    "print(\"━\" * 80)\n",
    "print(\"\\n Note: Training will take 15-30 minutes depending on your hardware.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c277cdb",
   "metadata": {},
   "source": [
    "## Part 6: Train the DCGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802220b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DCGAN (this takes time!)\n",
    "results = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    latent_dim=latent_dim,\n",
    "    log_interval=50,\n",
    ")\n",
    "\n",
    "print(\"\\n Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c97c6",
   "metadata": {},
   "source": [
    "## Part 7: Visualize Training Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Discriminator loss\n",
    "ax = axes[0]\n",
    "ax.plot(results[\"d_losses\"], linewidth=1.5, color=\"navy\", label=\"D Loss\")\n",
    "ax.axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Ideal (0.5)\")\n",
    "ax.set_xlabel(\"Training Step\", fontsize=11)\n",
    "ax.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax.set_title(\"Discriminator Loss Over Training\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Generator loss\n",
    "ax = axes[1]\n",
    "ax.plot(results[\"g_losses\"], linewidth=1.5, color=\"darkgreen\", label=\"G Loss\")\n",
    "ax.set_xlabel(\"Training Step\", fontsize=11)\n",
    "ax.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax.set_title(\"Generator Loss Over Training\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate statistics\n",
    "d_losses = np.array(results[\"d_losses\"])\n",
    "g_losses = np.array(results[\"g_losses\"])\n",
    "\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(\"━\" * 80)\n",
    "print(f\"Discriminator Loss:\")\n",
    "print(f\"  Initial: {d_losses[0]:.4f}\")\n",
    "print(f\"  Final:   {d_losses[-1]:.4f}\")\n",
    "print(f\"  Mean:    {d_losses.mean():.4f}\")\n",
    "print(f\"  Std:     {d_losses.std():.4f}\")\n",
    "print()\n",
    "print(f\"Generator Loss:\")\n",
    "print(f\"  Initial: {g_losses[0]:.4f}\")\n",
    "print(f\"  Final:   {g_losses[-1]:.4f}\")\n",
    "print(f\"  Mean:    {g_losses.mean():.4f}\")\n",
    "print(f\"  Std:     {g_losses.std():.4f}\")\n",
    "print(\"━\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb3018",
   "metadata": {},
   "source": [
    "## Part 8: Generate and Visualize Synthetic Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ce380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic images\n",
    "num_samples = 32\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, latent_dim, device=device)\n",
    "    synthetic_images = generator(z)\n",
    "\n",
    "# Denormalize synthetic images to [0, 1]\n",
    "synthetic_images_cpu = synthetic_images.cpu()\n",
    "synthetic_images_cpu = (synthetic_images_cpu + 1) / 2  # [-1, 1] → [0, 1]\n",
    "synthetic_images_cpu = torch.clamp(synthetic_images_cpu, 0, 1)\n",
    "\n",
    "# Visualize generated images\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "for i in range(32):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    img = synthetic_images_cpu[i].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"DCGAN Generated Synthetic Images from CIFAR-10\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Generated Images Summary:\")\n",
    "print(\"━\" * 80)\n",
    "print(f\"Number of Samples Generated: {num_samples}\")\n",
    "print(f\"Output Resolution: 32×32 pixels (RGB)\")\n",
    "print(f\"Image Range: [0, 1] (normalized)\")\n",
    "print(\"━\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f007f",
   "metadata": {},
   "source": [
    "## Part 9: Compare Real vs Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real and generated images side by side\n",
    "fig, axes = plt.subplots(4, 16, figsize=(20, 5))\n",
    "\n",
    "# Real images (top 2 rows)\n",
    "real_batch, _ = next(iter(train_loader))\n",
    "real_batch = (real_batch + 1) / 2  # Denormalize\n",
    "real_batch = torch.clamp(real_batch, 0, 1)\n",
    "\n",
    "for i in range(8):\n",
    "    # Real images\n",
    "    ax = axes[0, i]\n",
    "    ax.imshow(real_batch[i].permute(1, 2, 0).numpy())\n",
    "    ax.set_title(\"Real\", fontsize=9)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    ax = axes[1, i]\n",
    "    ax.imshow(real_batch[8 + i].permute(1, 2, 0).numpy())\n",
    "    ax.set_title(\"Real\", fontsize=9)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Generated images (bottom 2 rows)\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, latent_dim, device=device)\n",
    "    fake_batch = generator(z)\n",
    "\n",
    "fake_batch_cpu = (fake_batch.cpu() + 1) / 2\n",
    "fake_batch_cpu = torch.clamp(fake_batch_cpu, 0, 1)\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[2, i]\n",
    "    ax.imshow(fake_batch_cpu[i].permute(1, 2, 0).numpy())\n",
    "    ax.set_title(\"Generated\", fontsize=9, color=\"red\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    ax = axes[3, i]\n",
    "    ax.imshow(fake_batch_cpu[8 + i].permute(1, 2, 0).numpy())\n",
    "    ax.set_title(\"Generated\", fontsize=9, color=\"red\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Real CIFAR-10 Images vs DCGAN Generated Images\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Side-by-side comparison complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
