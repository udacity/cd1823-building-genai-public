{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395ea5f7",
   "metadata": {},
   "source": [
    "# Module 13: Implementing Conditional GANs (cGANs)\n",
    "\n",
    "**Learning Objective**: Understand and implement Conditional GANs that generate class-specific images on demand, with proper label conditioning in both Generator and Discriminator.\n",
    "\n",
    "## What is a cGAN?\n",
    "\n",
    "A **Conditional GAN** differs from standard GANs by:\n",
    "1. **Generator receives class label** - conditions image generation on target class\n",
    "2. **Discriminator receives class label** - enforces image-class consistency\n",
    "3. **Label concatenation** - combines noise + embedded label early in generator, image features + embedded label in discriminator\n",
    "4. **Class-specific synthesis** - can generate any class on demand\n",
    "\n",
    "Key Benefits:\n",
    "✓ Generate specific classes of images (e.g., \"generate a cat\")\n",
    "✓ Better control over generated content\n",
    "✓ Useful for data augmentation in classification tasks\n",
    "✓ Demonstrates label entanglement vs style variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31000c",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Import cGAN models from local files\n",
    "from models.cgan import (\n",
    "    ConditionalGenerator,\n",
    "    ConditionalDiscriminator,\n",
    "    create_cgan_models,\n",
    "    print_model_summary,\n",
    ")\n",
    "from cgan_training import ConditionalGANTrainer, initialize_weights\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Determine device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"All imports successful!\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\" PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ae207",
   "metadata": {},
   "source": [
    "## Part 2: Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # MPS compatibility\n",
    ")\n",
    "\n",
    "print(f\" CIFAR-10 loaded: {len(train_dataset)} images, {len(train_loader)} batches\")\n",
    "print(f\"  Classes: {CIFAR10_CLASSES}\")\n",
    "\n",
    "# Visualize sample images per class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 6))\n",
    "for i, class_idx in enumerate(range(10)):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    idx = np.where(np.array(train_dataset.targets) == class_idx)[0][0]\n",
    "    img = (train_dataset[idx][0] + 1) / 2\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(f\"{CIFAR10_CLASSES[class_idx]} (class {class_idx})\", fontweight=\"bold\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"CIFAR-10 Sample Images (One per Class)\", fontsize=13, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257a4aa",
   "metadata": {},
   "source": [
    "## Part 3: cGAN Architecture Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" CONDITIONING STRATEGIES IN cGAN\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. GENERATOR CONDITIONING:\")\n",
    "print(\"   Input: noise z (100,) + class label (one-hot)\")\n",
    "print(\"   Step 1: Embed class label: one-hot (10,) → continuous (50,)\")\n",
    "print(\"   Step 2: Concatenate: [z (100,) + embedded_label (50,)] → (150,)\")\n",
    "print(\"   Step 3: FC layers expand to 256×4×4 spatial features\")\n",
    "print(\"   Step 4: ConvTranspose layers upsample to 32×32 image\")\n",
    "print(\"   Result: Image that belongs to the target class\\n\")\n",
    "\n",
    "print(\"2. DISCRIMINATOR CONDITIONING:\")\n",
    "print(\"   Input: image (3, 32, 32) + class label (one-hot)\")\n",
    "print(\"   Step 1: Conv layers extract image features → (256, 2, 2)\")\n",
    "print(\"   Step 2: Embed class label: one-hot (10,) → continuous (50,)\")\n",
    "print(\"   Step 3: Flatten image features: (256×2×2=1024,)\")\n",
    "print(\"   Step 4: Concatenate: [image_features (1024,) + embedded_label (50,)]\")\n",
    "print(\"   Step 5: FC layers classify as real/fake\")\n",
    "print(\"   Result: Ensures image matches the claimed class\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n KEY INSIGHT:\")\n",
    "print(\"Both G and D know the target class:\")\n",
    "print(\"  • G tries to generate images of that class\")\n",
    "print(\"  • D checks if image belongs to that class\")\n",
    "print(\"  • Together they learn to disentangle class from style\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe10095",
   "metadata": {},
   "source": [
    "## Part 4: Create and Initialize cGAN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd744993",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "label_dim = 50\n",
    "num_channels = 3\n",
    "\n",
    "# Create cGAN models\n",
    "generator, discriminator = create_cgan_models(\n",
    "    latent_dim=latent_dim,\n",
    "    num_classes=num_classes,\n",
    "    label_dim=label_dim,\n",
    "    num_channels=num_channels,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)\n",
    "\n",
    "# Print model summary\n",
    "print_model_summary(generator, discriminator, latent_dim, num_classes)\n",
    "\n",
    "# Test forward passes\n",
    "print(\"\\n✓ Testing Forward Passes:\")\n",
    "print(\"=\" * 80)\n",
    "batch_size = 4\n",
    "z = torch.randn(batch_size, latent_dim, device=device)\n",
    "labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "\n",
    "fake_images = generator(z, labels)\n",
    "print(f\"Generator Input (noise):     {z.shape}\")\n",
    "print(f\"Generator Input (labels):    {labels.shape}\")\n",
    "print(f\"Generator Output (images):   {fake_images.shape}\")\n",
    "\n",
    "D_output = discriminator(fake_images, labels)\n",
    "print(f\"Discriminator Input (images): {fake_images.shape}\")\n",
    "print(f\"Discriminator Input (labels): {labels.shape}\")\n",
    "print(f\"Discriminator Output (prob):  {D_output.shape}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ All assertions passed!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e162a05",
   "metadata": {},
   "source": [
    "## Part 5: Train the cGAN Model\n",
    "\n",
    "⏱️ **Note:** Training will take 20-30 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3daaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# Create trainer\n",
    "trainer = ConditionalGANTrainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    device=device,\n",
    "    lr_g=0.0002,\n",
    "    lr_d=0.0002,\n",
    "    beta1=0.5,\n",
    "    beta2=0.999,\n",
    ")\n",
    "\n",
    "# Train\n",
    "results = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    latent_dim=latent_dim,\n",
    "    num_classes=num_classes,\n",
    "    log_interval=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0260b",
   "metadata": {},
   "source": [
    "## Part 6: Loss Curves and Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(results[\"d_losses\"], linewidth=1, color=\"navy\", label=\"D Loss\", alpha=0.7)\n",
    "ax.axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Ideal (0.5)\")\n",
    "ax.set_xlabel(\"Training Step\", fontsize=11)\n",
    "ax.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax.set_title(\"Discriminator Loss Over Training\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(results[\"g_losses\"], linewidth=1, color=\"darkgreen\", label=\"G Loss\", alpha=0.7)\n",
    "ax.set_xlabel(\"Training Step\", fontsize=11)\n",
    "ax.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax.set_title(\"Generator Loss Over Training\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "d_losses = np.array(results[\"d_losses\"])\n",
    "g_losses = np.array(results[\"g_losses\"])\n",
    "print(f\"\\n Training Statistics:\")\n",
    "print(f\"  D Loss - Mean: {d_losses.mean():.4f}, Std: {d_losses.std():.4f}\")\n",
    "print(f\"  G Loss - Mean: {g_losses.mean():.4f}, Std: {g_losses.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115167a",
   "metadata": {},
   "source": [
    "## Part 7: Generate Class-Specific Images (10×10 Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10x10 class grid (same noise, different classes)\n",
    "print(\"Generating 10×10 class grid (same noise z, different classes y)...\")\n",
    "print(\"This shows the cGAN's ability to disentangle class from other variations.\\n\")\n",
    "\n",
    "grid_images = trainer.generate_all_classes_grid(\n",
    "    num_classes=10,\n",
    "    samples_per_class=10,\n",
    "    latent_dim=latent_dim,\n",
    "    shared_z=None,  # Create a fixed noise vector\n",
    ")\n",
    "\n",
    "# Denormalize\n",
    "grid_images_cpu = (grid_images.cpu() + 1) / 2\n",
    "grid_images_cpu = torch.clamp(grid_images_cpu, 0, 1)\n",
    "\n",
    "# Visualize as grid\n",
    "fig, axes = plt.subplots(10, 10, figsize=(16, 16))\n",
    "for class_id in range(10):\n",
    "    for sample_id in range(10):\n",
    "        idx = class_id * 10 + sample_id\n",
    "        ax = axes[class_id, sample_id]\n",
    "        img = grid_images_cpu[idx].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "        if sample_id == 0:\n",
    "            ax.set_ylabel(CIFAR10_CLASSES[class_id], fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"cGAN Class Disentanglement: 10×10 Grid\\n(Rows: Classes 0-9 | Columns: Same Noise, Different Classes)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.995,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ 10×10 class grid generated successfully!\")\n",
    "print(f\"  Grid shape: {grid_images.shape}\")\n",
    "print(f\"  Each row represents one class\")\n",
    "print(f\"  Each column uses same noise but different class label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381470fc",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "1. **Conditional GANs (cGANs)** - Generate images of specific classes on demand\n",
    "2. **Label Conditioning** - How to incorporate class information in G and D\n",
    "3. **Class Disentanglement** - Separating class identity from style variation\n",
    "4. **10×10 Grid Technique** - Visualizing class control vs noise variation\n",
    "5. **Data Augmentation** - Using cGANs to generate synthetic training data\n",
    "\n",
    "### Key Differences from Unconditional GANs:\n",
    "- Generator takes class label → class-specific generation\n",
    "- Discriminator takes class label → enforces class consistency\n",
    "- Both use label embedding and concatenation\n",
    "- Training ensures label-image correspondence\n",
    "\n",
    "### Next Steps:\n",
    "- Train longer for better quality (50+ epochs)\n",
    "- Try with CIFAR-100 (100 classes)\n",
    "- Use generated images to augment classification datasets\n",
    "- Experiment with different label embedding dimensions\n",
    "- Combine with other GAN tricks (spectral normalization, Wasserstein loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "36371cb60c26e37b7d9a2ceed614c6abe3cd2e9c2c4d621fd25f98fd923082ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
