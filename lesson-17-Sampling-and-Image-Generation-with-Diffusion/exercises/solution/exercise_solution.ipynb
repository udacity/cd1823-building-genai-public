{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af670d0",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31c243",
   "metadata": {},
   "source": [
    "## Solution 1: Linear Noise Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb13a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule:\n",
    "    \"\"\"\n",
    "    Linear noise schedule implementation.\n",
    "\n",
    "    Spaces timesteps uniformly from 0 to T.\n",
    "    Simple and fast, but may not match optimal noise progression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise_scheduler, num_inference_steps: int = 50):\n",
    "        \"\"\"Initialize linear schedule with uniform timestep spacing.\"\"\"\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.num_steps = num_inference_steps\n",
    "\n",
    "        # Create linearly spaced timesteps (high to low)\n",
    "        timesteps = torch.linspace(\n",
    "            noise_scheduler.num_timesteps - 1,  # Start at T\n",
    "            0,  # End at 0\n",
    "            num_inference_steps,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        self.timesteps = timesteps.long()\n",
    "\n",
    "        # Pre-compute variances for each step\n",
    "        alphas_cumprod_prev = torch.cat(\n",
    "            [torch.ones(1), noise_scheduler.alphas_cumprod[:-1]]\n",
    "        )\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            noise_scheduler.betas\n",
    "            * (1.0 - alphas_cumprod_prev)\n",
    "            / (1.0 - noise_scheduler.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            self.posterior_variance.clamp(min=1e-20)\n",
    "        )\n",
    "\n",
    "        # Coefficients for posterior mean\n",
    "        self.posterior_mean_coef1 = (\n",
    "            noise_scheduler.betas\n",
    "            * torch.sqrt(alphas_cumprod_prev)\n",
    "            / (1.0 - noise_scheduler.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - alphas_cumprod_prev)\n",
    "            * torch.sqrt(noise_scheduler.alphas)\n",
    "            / (1.0 - noise_scheduler.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    def get_timestep(self, step: int) -> int:\n",
    "        \"\"\"Get timestep for given step number.\"\"\"\n",
    "        return self.timesteps[step]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343ccf8",
   "metadata": {},
   "source": [
    "## Solution 2: Cosine Noise Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ada00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule:\n",
    "    \"\"\"\n",
    "    Cosine noise schedule implementation.\n",
    "\n",
    "    Uses cosine function to create non-linear timestep spacing.\n",
    "    Focuses more computational effort on high-noise (difficult) steps.\n",
    "    Often produces better quality with fewer total steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise_scheduler, num_inference_steps: int = 50):\n",
    "        \"\"\"Initialize cosine schedule with non-linear timestep spacing.\"\"\"\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.num_steps = num_inference_steps\n",
    "\n",
    "        # Create cosine-spaced timesteps\n",
    "        s = 0.008  # Offset parameter\n",
    "        steps = torch.arange(num_inference_steps + 1, dtype=torch.float32)\n",
    "\n",
    "        # Cosine schedule formula\n",
    "        alphas_cumprod = (\n",
    "            torch.cos((steps / num_inference_steps + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "        )\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "\n",
    "        # Find nearest actual timesteps for each cosine point\n",
    "        timesteps = []\n",
    "        for i in range(num_inference_steps):\n",
    "            # Target alpha at this step\n",
    "            target_alpha = alphas_cumprod[i + 1]\n",
    "            # Find closest timestep with this alpha\n",
    "            t = torch.argmin(torch.abs(noise_scheduler.alphas_cumprod - target_alpha))\n",
    "            timesteps.append(t)\n",
    "\n",
    "        # Reverse (high noise to low noise)\n",
    "        self.timesteps = torch.tensor(timesteps[::-1], dtype=torch.long)\n",
    "\n",
    "        # Pre-compute variances (same as linear)\n",
    "        alphas_cumprod_prev = torch.cat(\n",
    "            [torch.ones(1), noise_scheduler.alphas_cumprod[:-1]]\n",
    "        )\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            noise_scheduler.betas\n",
    "            * (1.0 - alphas_cumprod_prev)\n",
    "            / (1.0 - noise_scheduler.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            self.posterior_variance.clamp(min=1e-20)\n",
    "        )\n",
    "\n",
    "        self.posterior_mean_coef1 = (\n",
    "            noise_scheduler.betas\n",
    "            * torch.sqrt(alphas_cumprod_prev)\n",
    "            / (1.0 - noise_scheduler.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - alphas_cumprod_prev)\n",
    "            * torch.sqrt(noise_scheduler.alphas)\n",
    "            / (1.0 - noise_scheduler.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    def get_timestep(self, step: int) -> int:\n",
    "        \"\"\"Get timestep for given step number.\"\"\"\n",
    "        return self.timesteps[step]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a4b34",
   "metadata": {},
   "source": [
    "## Solution 3: Reverse Diffusion Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion_step(\n",
    "    model_output: torch.Tensor,\n",
    "    timestep: int,\n",
    "    sample: torch.Tensor,\n",
    "    noise_scheduler,\n",
    "    schedule,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Single reverse diffusion step.\n",
    "\n",
    "    Takes one step from x_t toward x_{t-1} using the model's noise prediction.\n",
    "\n",
    "    Args:\n",
    "        model_output: Predicted noise from U-Net (batch, 1, 28, 28)\n",
    "        timestep: Current timestep t\n",
    "        sample: Current noisy image x_t (batch, 1, 28, 28)\n",
    "        noise_scheduler: Base scheduler with coefficients\n",
    "        schedule: LinearSchedule or CosineSchedule\n",
    "\n",
    "    Returns:\n",
    "        Denoised sample x_{t-1}\n",
    "    \"\"\"\n",
    "    # Get alpha coefficients\n",
    "    alpha_t = noise_scheduler.alphas_cumprod[timestep]\n",
    "    alpha_t_prev = (\n",
    "        noise_scheduler.alphas_cumprod[timestep - 1] if timestep > 0 else torch.ones(1)\n",
    "    )\n",
    "\n",
    "    # Predicted original image\n",
    "    pred_original_sample = (\n",
    "        sample - torch.sqrt(1 - alpha_t) * model_output\n",
    "    ) / torch.sqrt(alpha_t)\n",
    "\n",
    "    # Posterior mean\n",
    "    coef1 = schedule.posterior_mean_coef1[timestep]\n",
    "    coef2 = schedule.posterior_mean_coef2[timestep]\n",
    "    mean = coef1 * pred_original_sample + coef2 * sample\n",
    "\n",
    "    # Add variance (stochastic sampling)\n",
    "    variance = schedule.posterior_variance[timestep]\n",
    "    if variance > 0:\n",
    "        z = torch.randn_like(sample)\n",
    "        sample = mean + torch.sqrt(variance) * z\n",
    "    else:\n",
    "        sample = mean\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd3862",
   "metadata": {},
   "source": [
    "## Solution 4: Complete Sampling Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_schedule(\n",
    "    model: nn.Module,\n",
    "    noise_scheduler,\n",
    "    schedule,\n",
    "    batch_size: int = 4,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Complete reverse diffusion sampling loop.\n",
    "\n",
    "    Process:\n",
    "    1. Initialize with pure Gaussian noise\n",
    "    2. Iterate from high noise to low noise\n",
    "    3. At each step:\n",
    "       - Predict noise with model\n",
    "       - Take reverse step\n",
    "       - Store for visualization\n",
    "    4. Return final images and trajectory\n",
    "\n",
    "    Args:\n",
    "        model: Trained U-Net denoiser\n",
    "        noise_scheduler: Base scheduler\n",
    "        schedule: LinearSchedule or CosineSchedule\n",
    "        batch_size: Number of images to generate\n",
    "        device: torch.device (cuda/mps/cpu)\n",
    "\n",
    "    Returns:\n",
    "        samples: Generated images (batch, 1, 28, 28)\n",
    "        trajectory: List of intermediate samples\n",
    "    \"\"\"\n",
    "    # Initialize with pure Gaussian noise\n",
    "    sample = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "    trajectory = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Iterate through timesteps (high noise to low noise)\n",
    "        schedule_name = \"Linear\" if isinstance(schedule, LinearSchedule) else \"Cosine\"\n",
    "\n",
    "        for i, t in enumerate(\n",
    "            tqdm(schedule.timesteps, desc=f\"Sampling ({schedule_name})\")\n",
    "        ):\n",
    "            # Prepare timestep tensor\n",
    "            t_tensor = torch.full(\n",
    "                (batch_size,),\n",
    "                t.item() if isinstance(t, torch.Tensor) else t,\n",
    "                device=device,\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "\n",
    "            # Predict noise with U-Net\n",
    "            noise_pred = model(sample, t_tensor)\n",
    "\n",
    "            # Take reverse diffusion step\n",
    "            sample = reverse_diffusion_step(\n",
    "                noise_pred, t, sample, noise_scheduler, schedule\n",
    "            )\n",
    "\n",
    "            # Store intermediate results\n",
    "            if i % max(1, len(schedule.timesteps) // 5) == 0:\n",
    "                trajectory.append(sample.cpu().clone())\n",
    "\n",
    "    return sample, trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b5aa9",
   "metadata": {},
   "source": [
    "## Solution 5: Fidelity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_variance(samples: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute variance of generated samples across batch.\n",
    "\n",
    "    Low variance → all samples are similar (potential mode collapse)\n",
    "    High variance → diverse samples (good exploration)\n",
    "\n",
    "    Args:\n",
    "        samples: (batch, channels, H, W)\n",
    "\n",
    "    Returns:\n",
    "        Variance value\n",
    "    \"\"\"\n",
    "    # Flatten to (batch, -1)\n",
    "    flat_samples = samples.reshape(samples.shape[0], -1)\n",
    "\n",
    "    # Compute variance across batch\n",
    "    sample_mean = flat_samples.mean(dim=0, keepdim=True)\n",
    "    variance = ((flat_samples - sample_mean) ** 2).mean()\n",
    "\n",
    "    return variance.item()\n",
    "\n",
    "\n",
    "def compare_schedules(model, noise_scheduler, device, batch_size=16):\n",
    "    \"\"\"\n",
    "    Compare Linear vs Cosine sampling schedules.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for schedule_class, schedule_name in [\n",
    "        (LinearSchedule, \"Linear\"),\n",
    "        (CosineSchedule, \"Cosine\"),\n",
    "    ]:\n",
    "        print(f\"\\nTesting {schedule_name} Schedule...\")\n",
    "\n",
    "        # Create schedule\n",
    "        schedule = schedule_class(noise_scheduler, num_inference_steps=50)\n",
    "\n",
    "        # Sample\n",
    "        start_time = time.time()\n",
    "        samples, trajectory = sample_with_schedule(\n",
    "            model, noise_scheduler, schedule, batch_size, device\n",
    "        )\n",
    "        sampling_time = time.time() - start_time\n",
    "\n",
    "        # Compute metrics\n",
    "        variance = compute_sample_variance(samples)\n",
    "\n",
    "        results[schedule_name] = {\n",
    "            \"samples\": samples,\n",
    "            \"trajectory\": trajectory,\n",
    "            \"time\": sampling_time,\n",
    "            \"variance\": variance,\n",
    "        }\n",
    "\n",
    "        print(f\"  Time: {sampling_time:.2f}s\")\n",
    "        print(f\"  Variance: {variance:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ca2de",
   "metadata": {},
   "source": [
    "## Solution 6: Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f23dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sampling_comparison(results):\n",
    "    \"\"\"\n",
    "    Visualize samples and metrics from both schedules.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "    for idx, (schedule_name, data) in enumerate(results.items()):\n",
    "        samples = (data[\"samples\"][:8] + 1) / 2  # Denormalize\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = axes[idx, i]\n",
    "            ax.imshow(sample.squeeze().numpy(), cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # Add title on left\n",
    "        axes[idx, 0].text(\n",
    "            -1.5,\n",
    "            0.5,\n",
    "            schedule_name,\n",
    "            transform=axes[idx, 0].transAxes,\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "            ha=\"right\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Sampling Schedule Comparison: Generated MNIST Digits\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_schedule_analysis(results):\n",
    "    \"\"\"\n",
    "    Print detailed analysis of schedule comparison.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLING SCHEDULE COMPARISON RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\n1. SPEED COMPARISON:\")\n",
    "    for name, data in results.items():\n",
    "        print(f\"   {name}: {data['time']:.3f} seconds\")\n",
    "\n",
    "    print(\"\\n2. DIVERSITY (Sample Variance):\")\n",
    "    for name, data in results.items():\n",
    "        print(f\"   {name}: {data['variance']:.6f}\")\n",
    "\n",
    "    print(\"\\n3. RECOMMENDATIONS:\")\n",
    "    print(\"   • Linear Schedule: Simple, predictable, good baseline\")\n",
    "    print(\"   • Cosine Schedule: Often higher quality, focuses on hard steps\")\n",
    "    print(\"   • Choice depends on quality vs speed tradeoff\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
